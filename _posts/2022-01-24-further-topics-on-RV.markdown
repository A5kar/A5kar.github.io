---
layout: post
title: "Further topics on r.v."
---

## Derived distributions {#derived_distributions}

Probability law on $$Y$$, where $$Y=g(X)$$ depends on whether $$X$$ is discrete or continuous r.v.{% if jekyll.environment == "development" %} (see [Prob L11 Q5](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__11_Derived_distributions/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s2-tab5)){% endif %}. In the first case, our computations are based on the fact that $$p_Y(y)=\mathbb P(g(X)=y)$$. Else, we follow a two-step procedure: find $$F_Y(y)=\mathbb P(g(X)\le y)$$, and differentiate $$f_Y(y)=\frac{\partial}{\partial y}F_Y(y)$${% if jekyll.environment == "development" %} (see [Prob MT2 Q1](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@sequential_Exam_2/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch11-s1-tab2), [Prob MT2 Q3](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@sequential_Exam_2/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch11-s1-tab4) and [Prob MT2 Q5](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@sequential_Exam_2/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch11-s1-tab6)){% endif %}.

|$$g(X)$$|$$X$$ discrete|$$X$$ continuous|
|:-:|:-:|:-:|
|linear<br>$$g(X)=aX+b$$|$$\displaystyle p_Y(y)=p_X\left(\frac{y-b}{a}\right)$${% if jekyll.environment == "development" %}<br>(see [Prob L11 Q3](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__11_Derived_distributions/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s2-tab3)){% endif %}|$$\displaystyle f_Y(y)=f_X\left(\frac{y-b}{a}\right)\frac{1}{\lvert a\rvert}$$|
|strictly monotonic<br>(hence invertible)<br>$$X=h(Y)$$|$$\displaystyle p_Y(y)=p_X(h(y))$$|$$\displaystyle f_Y(y)=f_X(h(y))\left\lvert\frac{\partial}{\partial y}h(y)\right\rvert$${% if jekyll.environment == "development" %}<br>(see [Prob L11 Q8](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__11_Derived_distributions/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s2-tab8), [Prob L11 Q10](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__11_Derived_distributions/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s2-tab10) and [Prob PS6 Q1](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Problem_Set_6/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s7-tab1)){% endif %}|
|general|$$\displaystyle p_Y(y)=\sum_{x:g(x)=y}p_X(x)$$|$$\displaystyle f_Y(y)=\sum_{h_i(y):g(h_i(y))=y}f_X(h_i(y))\left\lvert\frac{\partial}{\partial y}h_i(y)\right\rvert$${% if jekyll.environment == "development" %}<br>(see [Prob L11 Q13](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__11_Derived_distributions/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s2-tab13) and [Prob PS6 Q2](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Problem_Set_6/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s7-tab2)){% endif %}|

Note that if $$X$$ is uniform (irrespective if discrete or continuous), a linear combination $$Y=aX+b$$ is still uniform, just with a different *scale* and *location*{% if jekyll.environment == "development" %} (see [Prob PS5 Q4](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Problem_Set_5/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch8-s7-tab4) and [Stats HW0 Q4](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@prob_linalg_diag/block-v1:MITx+18.6501x+2T2021+type@vertical+block@prob_linalg_diag-tab4)){% endif %}.

The general continuous case assumes that the domain can be split into intervals within which $$g(X)$$ is still **strictly** monotonic, this to ensure that $$\nexists y:\mathbb P(Y=y)\neq0$$. Otherwise, we would fall into a mixed discrete/continuous r.v. case.

Finally, observe that if $$X\sim\mathcal N(\mu_X,\sigma_X^2)$$ then for any $$Y=aX+b$$, we have that $$Y\sim\mathcal N(a\mu_X+b,a^2\sigma_X^2)$$.

## Simulation

An approach similar to derived distributions can be used to simulate *any* distribution, starting from $$U\sim\text{Unif}(0,1)$$. In particular, by observing that $$\mathbb P(U\le F_X(x))=F_X(x)$$, we can adopt the following solutions, depending on whether $$X$$ is discrete or continuous.

- **Discrete case**: we assign to $$X$$ the $$k$$-th value that satisfies $$F_X(k-1)\lt u\le F_X(k)$$; and
- **Continuous case**: we assign to $$X$$ that $$x$$ that satisfies $$u=F_X(x)$$.

## Function of multiple r.v.

Calculating probability law on $$Z=g(X,Y)$$ is not different from the single r.v. case. If $$f_{Y\lvert X}(y\lvert x)$$ is available, and there is a way to represent $$Y=h(X,Z)$$, we compute $$f_{Z\lvert X}(z\lvert x)$$ with the exactly same procedure, bearing in mind that $$X=x$$ is given. In other words, first we compute $$F_{Z\lvert X}(z\lvert x)=F_{Y\lvert X}(h(x,z)\lvert x)$$, and then differentiate $$f_{Z\lvert X}(z\lvert x)=f_{Y\lvert X}(h(x,z)\lvert x)\left\lvert\frac{\partial}{\partial z}h(x,z)\right\rvert$$. At this point, total probability theorem tells us that $$f_Z(z)=\int_{-\infty}^\infty f_X(x)f_{Z\lvert X}(z\lvert x)dx$$.

Sometimes, calculations can be simplified. Assume $$f_{X,Y}(x,y)=c$$ is constant and $$Z=g(X,Y)$$ is associated with area $$A(Z)$$ in the $$X-Y$$ plane. In this case, $$F_Z(z)=\mathbb P(g(X,Y)\le z)=cA(z)$${% if jekyll.environment == "development" %} (see [Prob L11 Q15](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__11_Derived_distributions/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s2-tab15)){% endif %}.

For $$X$$ and $$Y$$ independent, we have $$\mathbb P(\max\{X,Y\}\le z)=F_X(z)F_Y(z)$${% if jekyll.environment == "development" %} (see [Prob PS6 Q3](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Problem_Set_6/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s7-tab3) and [Prob F Q4](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@sequential_Final_Exam/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch16-s1-tab5)){% endif %}, and $$\mathbb P(\min\{X,Y\}\le z)=1-(1-F_X(z))(1-F_Y(z))$$.

## Sum of independent r.v.

From the above it follows that depending on whether we are dealing with discrete or continuous r.v., for $$Z=X+Y$$ ($$X$$, $$Y$$ independent), we have $$p_{Z\lvert X}(z\lvert x)=p_Y(z-x)$$, or $$f_{Z\lvert X}(z\lvert x)=f_Y(z-x)$$. Therefore, the probability law on $$Z$$ can be either $$p_Z(z)=\sum_x p_X(x)p_Y(z-x)$${% if jekyll.environment == "development" %} (see [Prob L12 Q3](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__12_Sums_of_independent_r_v_s__Covariance_and_correlation/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s3-tab3)){% endif %} or $$f_Z(z)=\int_{-\infty}^\infty f_X(x)f_Y(z-x)dx$${% if jekyll.environment == "development" %} (see [Prob L12 Q5](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__12_Sums_of_independent_r_v_s__Covariance_and_correlation/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s3-tab5) and [Prob PS6 Q4](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Problem_Set_6/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s7-tab4)){% endif %}, respectively. Due to its widespread use, the aforementioned formula is referred to as [convolution](https://en.wikipedia.org/wiki/Convolution).

For the general case $$Z=aX+bY$$, where $$a,b\in\mathbb R$$, we get either $$p_Z(z)=\sum_x p_X(\frac{x}{a})p_Y(\frac{z-x}{b})$$ or $$f_Z(z)=\int_{-\infty}^\infty f_X(\frac{x}{a})\frac{1}{\lvert a\rvert}f_Y(\frac{z-x}{b})\frac{1}{\lvert b\rvert}dx$$, depending on whether $$X$$ and $$Y$$ are discrete or continuous.

In case $$X\sim\mathcal N(\mu_X,\sigma_X^2)$$ and $$Y\sim\mathcal N(\mu_Y,\sigma_Y^2)$$, we have that $$Z\sim\mathcal N(\mu_X+\mu_Y,\sigma_X^2+\sigma_Y^2)$$, [proving](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables) once again linearity of Normal r.v.{% if jekyll.environment == "development" %} (see [Prob L12 Q7](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__12_Sums_of_independent_r_v_s__Covariance_and_correlation/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s3-tab7)){% endif %}.

## Covariance {#covariance}

Defined as $$\sigma_{XY}=\text{cov}(X,Y)=\mathbb E[(X-\mathbb E[X])(Y-\mathbb E[Y])]$$, covariance is a measure of the joint variability of two r.v.{% if jekyll.environment == "development" %} (see [Prob L12 Q11](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__12_Sums_of_independent_r_v_s__Covariance_and_correlation/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s3-tab11)){% endif %}.

|Property|Formula|
|-|:-:|
|Variance|$$\text{cov}(X,X)=\text{var}(X)$$|
|Commutative|$$\text{cov}(X,Y)=\text{cov}(Y,X)$$|
|Bilinear|$$\text{cov}(aX,bY+cZ)=ab\text{cov}(X,Y)+ac\text{cov}(X,Z)$${% if jekyll.environment == "development" %} (see [Stats HW0 Q3](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@prob_linalg_diag/block-v1:MITx+18.6501x+2T2021+type@vertical+block@prob_linalg_diag-tab3)){% endif %}|
|Variance|$$\displaystyle\text{var}\left(\sum_{i=1}^n a_iX_i\right)=\underbrace{\sum_{i=1}^n a_i^2\text{var}(X_i)}_{\text{$n$ terms}}+\underbrace{\sum_{i\neq j}a_ia_j\text{cov}(X_i,X_j)}_{\text{$n^2-n$ terms}}$${% if jekyll.environment == "development" %}<br>(see [Prob L12 Q13](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__12_Sums_of_independent_r_v_s__Covariance_and_correlation/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s3-tab13)){% endif %}<br>$$\text{var}(aX+bY)=a^2\text{var}(X)+b^2\text{var}(Y)+2ab\text{cov}(X,Y)$$|
|Equivalency|$$\text{cov}(X,Y)=\mathbb E[XY]-\mathbb E[X]\mathbb E[Y]$${% if jekyll.environment == "development" %}<br>(see [Prob L12 Q9](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__12_Sums_of_independent_r_v_s__Covariance_and_correlation/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s3-tab9), [Prob PS6 Q5](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Problem_Set_6/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s7-tab5) and [Prob F Q3](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@sequential_Final_Exam/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch16-s1-tab4)){% endif %}|

If $$X$$ and $$Y$$ are independent, then $$\text{cov}(X,Y)=0$$; the converse, however, is not true in general{% if jekyll.environment == "development" %} (see [Prob MT2 Q1](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@sequential_Exam_2/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch11-s1-tab2)){% endif %}.

## Correlation

Correlation is a measure of the degree of **linear** association, whether causal or not, between two r.v. and is defined as $$\rho_{XY}=\mathbb E\left[\left(\frac{X-\mathbb E[X]}{\sigma_X}\right)\left(\frac{Y-\mathbb E[Y]}{\sigma_Y}\right)\right]=\frac{\text{cov}(X,Y)}{\sigma_X\sigma_Y}$${% if jekyll.environment == "development" %} (see [Prob L12 Q15](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__12_Sums_of_independent_r_v_s__Covariance_and_correlation/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s3-tab15)){% endif %}.

Main properties of the correlation coefficient are that $$\lvert\rho\rvert\le1$$ and that $$\lvert\rho\rvert=1\implies Y=aX$$, with $$a$$ constant, while others can be derived directly from covariance{% if jekyll.environment == "development" %} (see [Prob L12 Q18](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__12_Sums_of_independent_r_v_s__Covariance_and_correlation/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s3-tab18) and [Prob PS6 Q6](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Problem_Set_6/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s7-tab6)){% endif %}.

## Conditional expectation as r.v.

So far, we saw that for any r.v. $$X$$, $$\mathbb E[X]$$ is a number. Same thing happens with conditional expectation, as $$g(y)=\mathbb E[X\lvert Y=y]$$ is still a number, which depends on parameter $$y$$. However, since any function of a r.v. is a r.v. itself, it follows that $$g(Y)=\mathbb E[X\lvert Y]$$ is a r.v., with its own distribution, expectation and variance{% if jekyll.environment == "development" %} (see [Prob L13 Q3](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__13_Conditional_expectation_and_variance_revisited__Sum_of_a_random_number_of_independent_r_v_s/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s4-tab3) and [Prob L13 Q7](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__13_Conditional_expectation_and_variance_revisited__Sum_of_a_random_number_of_independent_r_v_s/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s4-tab7)){% endif %}. Same considerations apply to any other function $$\mathbb E[h(X)\lvert Y]$$, such as $$\text{var}(X\lvert Y)=\mathbb E[(X-\mathbb E[X\lvert Y])^2\lvert Y]$$, where the inner $$\mathbb E[X\lvert Y]$$ reminds us that all expectations are conditional on $$Y$${% if jekyll.environment == "development" %} (see [Prob L13 Q10](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__13_Conditional_expectation_and_variance_revisited__Sum_of_a_random_number_of_independent_r_v_s/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s4-tab10), [Prob L13 Q11](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__13_Conditional_expectation_and_variance_revisited__Sum_of_a_random_number_of_independent_r_v_s/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s4-tab11), [Prob L13 Q15](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__13_Conditional_expectation_and_variance_revisited__Sum_of_a_random_number_of_independent_r_v_s/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s4-tab15) and [Prob MT2 Q2](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@sequential_Exam_2/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch11-s1-tab3)){% endif %}.

Assuming for simplicity discrete case, we observe that in accordance with the [total expectation theorem](/2022/01/08/discrete-random-variables.html#conditioning) we have that $$\mathbb E[\mathbb E[X\lvert Y]]=\sum_y p_Y(y)\mathbb E[X\lvert Y=y]=\mathbb E[X]$${% if jekyll.environment == "development" %} (see [Prob L13 Q5](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__13_Conditional_expectation_and_variance_revisited__Sum_of_a_random_number_of_independent_r_v_s/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s4-tab5)){% endif %}. This is known as the **law of total expectation**, also referred to as the *law of iterated expectations*.

Analogously, one can derive $$\text{var}(X)=\mathbb E[\text{var}(X\lvert Y)]+\text{var}(\mathbb E[X\lvert Y])$$ known as the **law of total variance**. One way to remember the formula is that it calculates the average variance within sections and variance between sections.

A very common situation is when $$X=\sum_{i=1}^N X_i$$, with $$X_i\overset{\text{i.i.d.}}{\sim}\mathcal P$$, and $$N$$ another, independent r.v. In this scenario, $$\mathbb E[X]=\mathbb E[N]\mathbb E[X_i]$$ and $$\text{var}(X)=\mathbb E[N]\text{var}(X_i)+(\mathbb E[X_i])^2\text{var}(N)$${% if jekyll.environment == "development" %} (see [Prob L13 Q18](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__13_Conditional_expectation_and_variance_revisited__Sum_of_a_random_number_of_independent_r_v_s/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s4-tab18), [Prob PS6 Q7](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Problem_Set_6/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch9-s7-tab7) and [Prob F Q5](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@sequential_Final_Exam/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch16-s1-tab6)){% endif %}.

Final remark is that $$\mathbb E[g(Y)X\lvert Y]=g(Y)\mathbb E[X\lvert Y]$$, which comes from the observation that given $$Y=y$$, $$g(Y)$$ is a constant and can be pulled out of the expectation. Also, for any invertible function $$h(Y)$$, we have that $$\mathbb E[X\lvert Y]=\mathbb E[X\lvert h(Y)]$$, due to the fact that $$\mathbb P(Y\le y)=\mathbb P(h(Y)\le h(y))$$.

Go back to the [syllabi breakdown](/2022/01/02/prob-and-stats-syllabi.html).
