---
layout: post
title: "Foundation of Inference"
---

## Trinity of statistical inference

Goal of statistics is to learn the distribution of $$X$$ after having observed $$n$$ (eventually independent) copies of it. To achieve this, we introduce mathematical formalization of *statistical modeling* to make principled sense of the following three statements:

- Define a function providing an *estimation* of the unknown parameter based on the observations;
- Set a *Confidence Interval* (CI) where the unknown parameter will fall with given probability; and
- *Test* the existence of statistical evidence that the unknown parameter satisfies certain hypothesis.

## Statistical model

Let $$X_1,\dots,X_n\overset{i.i.d.}{\sim}\mathcal P$$ be the observed outcome of a *statistical experiment*. In this setting, **statistical model** associated with such statistical experiment is the following pair.

$$(E,\{\mathcal P_\theta\}_{\theta\in\Theta})$$

Where:

- $$E$$ is the *sample space* (usually $$E\subseteq\mathbb R^n$$);
- $$\{\mathcal P_\theta\}_{\theta\in\Theta}$$ is a *family* of probability measures on $$E$$; and
- $$\Theta$$ is the *parameter set*.

Note that when $$\exists\theta^\star$$ s.t. $$\mathcal P_{\theta^\star}=\mathcal P$$, then $$\theta^\star$$ is referred to as the (unknown) **true parameter** and the model is **well-specified**.

Otherwise the model is **mis-specified** and no matter how much data you collect, you may never be able to replicate the exact probability distribution $$\mathcal P$$. In reality, most assumptions necessarily lead to mis-specified models, e.g. to avoid otherwise intractable analysis of the data. Hence, the saying [all models are wrong, but some are useful](https://en.wikipedia.org/wiki/All_models_are_wrong) and the main take here is that although we may never get the *true* underlying distribution, we should aim at a good approximation that achieves our objectives. Another conclusion is that when competing models explain a given phenomenon, we should prefer the simpler one (e.g. a model with fewer parameters and smaller sample space){% if jekyll.environment == "development" %} (see [Stats L3 Q4](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s01_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s01_parainference-tab4)){% endif %}.

When $$\Theta\subseteq\mathbb R^d$$ and $$1\le d\lt\infty$$, we say that the model is **parametric**{% if jekyll.environment == "development" %} (see [Stats L3 Q8](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s01_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s01_parainference-tab8)){% endif %}, otherwise if $$d=\infty$$, the model is **non-parametric**. For completeness, statistical model is *semi-parametric* when $$\Theta=\Theta_1\times\Theta_2$$, which means that $$\Theta$$ can be split into finite dimensional $$\Theta_1$$ and infinite dimensional $$\Theta_2$$ (referred to as *nuisance* parameter). In that case, one  only cares to estimate $$\theta\in\Theta_1$$.

Below, an *indicative* list of statistical models for the basic [discrete](/2022/01/08/discrete-random-variables.html#basic) and [continuous](/2022/01/13/continuous-random-variables.html#basic) distributions{% if jekyll.environment == "development" %}(see [Stats L3 Q5](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s01_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s01_parainference-tab5) and [Stats L3 Q7](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s01_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s01_parainference-tab7)){% endif %}.

|Distribution and unknown parameter|Statistical model|
|-|:-:|
|Bernoulli with parameter $$p$$|$$\left(\{0,1\},\{\text{Ber}(p)\}_{p\in[0,1]}\right)$$|
|Binomial with parameter $$p$$|$$\left(\{0,1,\dots,n\},\{\text{Bin}(n,p)\}_{p\in[0,1]}\right)$$|
|Geometric with parameter $$p$$|$$\left(\mathbb Z_{\ge1},\{\text{Geom}(p)\}_{p\in[0,1]}\right)$$|
|Poisson with parameter $$\lambda$$|$$\left(\mathbb Z_{\ge0},\{\text{Pois}(\lambda)\}_{\lambda>0}\right)$$|
|Continuous uniform over $$[0,\theta]$$|$$\left(\mathbb R_{\ge0},\{\text{Unif}(0,\theta)\}_{\theta>0}\right)$$|
|Exponential with parameter $$\lambda$$|$$\left(\mathbb R_{\ge0},\{\text{Exp}(\lambda)\}_{\lambda>0}\right)$$|
|Normal with parameters $$(\mu,\sigma^2)$$|$$\left(\mathbb R,\{\mathcal N(\mu,\sigma^2)\}_{(\mu,\sigma^2)\in\mathbb R\times(0,\infty))}\right)$$|
|Multivariate Normal with parameter $$\mu$$|$$\left(\mathbb R^n,\{\mathcal N(\mu,\Sigma)\}_{\mu\in\mathbb R^n}\right)$${% if jekyll.environment == "development" %}<br>(see [Stats L6 Q3](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s04_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s04_parainference-tab3)){% endif %}|
|Linear regression model|$$(X_i,Y_i)\subseteq\mathbb R^d\times\mathbb R$$ are i.i.d. from $$Y_i=X_i\beta+\varepsilon_i$$<br>where $$X_i\sim\mathcal N(0,I_d)$$ and $$\varepsilon_i\overset{i.i.d.}{\sim}\mathcal N(0,1)$$<br>for an unknown  $$\beta\in\mathbb R^d$${% if jekyll.environment == "development" %} (see [Stats L3 Q9](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s01_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s01_parainference-tab9)){% endif %}|

While setting up the statistical model, we should ask ourselves if the parameter $$\theta$$ is identifiable, which is true *if and only if* $$\theta\neq\theta^\star\implies\mathcal P_\theta\neq\mathcal P_{\theta^\star}$$, or equivalently $$\mathcal P_\theta=\mathcal P_{\theta^\star}\implies\theta=\theta^\star$${% if jekyll.environment == "development" %} (see [Stats L3 Q10](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s01_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s01_parainference-tab10), [Stats L3 Q11](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s01_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s01_parainference-tab11) and [Stats HW1 Q1](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw1_u1intro/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw1_u1intro-tab1)){% endif %}.

## Estimation

Any quantity computed from values in a sample is called a **statistic**. Common examples are:

- sample mean ($$\bar X_n$$), sample mode ($$\arg\max f_X(x)$$);
- sample variance (e.g. biased $$\sigma^2$$, unbiased $$s^2$$);
- order statistics (e.g. maximum $$X_{(n)}$$, minimum $$X_{(1)}$$, median $$X_{\left(\frac 1 2 n\right)}$$);
- test statistics (e.g. $$z$$-statistic, $$t$$-statistic, $$\chi^2$$-statistic, $$F$$-statistic); and
- in general, any *measurable* (i.e. computable) function of the sample.

When a statistic is used for estimation of a population parameter, it is called an **estimator**. Let $$\hat\Theta_n$$ be an estimator of parameter $$\theta$$, which depends on the number of samples $$n$$. Estimator is called (strongly or weakly) **consistent** when it converges (almost surely or in probability) to the true parameter $$\theta$$, that is $$\hat\Theta\xrightarrow{a.s./\mathbb P}\theta$$. Also, if $$\sqrt{n}(\hat\Theta_n-\theta)\xrightarrow{(d)}\mathcal N(0,\sigma^2)$$, then the estimator is *asymptotically* Normal, and $$\sigma^2$$ is the *asymptotic* variance of $$\hat\Theta_n$${% if jekyll.environment == "development" %} (see [Stats L4 Q2](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s02_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s02_parainference-tab2) and Stats HW1 Q3){% endif %}.

Below are examples of some empirical (or natural) estimators, where the term "[empirical](https://en.wikipedia.org/wiki/Empirical_distribution_function)" stands for observation of **relative frequency** obtained from sample of data. An important property of any estimator is **bias**, or $$\text{bias}(\hat\Theta_n)=\mathbb E[\hat\Theta_n]-\theta$${% if jekyll.environment == "development" %} (see [Stats L4 Q3](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s02_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s02_parainference-tab3)){% endif %}. Except for expectation, all below empirical (or natural) estimators are *biased*, hence adjustments have been applied{% if jekyll.environment == "development" %} (see [Stats HW1 Q2](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw1_u1intro/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw1_u1intro-tab2)){% endif %}.

|Property|Empirical/Natural estimator|Alternative/Unbiased estimator|
|-|:-:|:-:|
|expectation $$\mathbb E[X_i]$$|$$\displaystyle\bar X_n=\frac 1 n \sum_{i=1^n}X_i$$|$$\displaystyle\bar X_n=\frac 1 n \sum_{i=1^n}X_i$$|
|expectation of a function $$\mathbb E[f(X_i)]$$|$$\displaystyle\bar X_n=\frac 1 n \sum_{i=1^n}f(X_i)$$|depends on $$f(X_i)$${% if jekyll.environment == "development" %}<br>(see [Prob L20 Q14](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__20_An_introduction_to_classical_statistics/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch12-s4-tab14)){% endif %}|
|variance $$\text{var}(X_i)$$|$$\displaystyle\frac 1 n\sum_{i=1}^n (X_i-\bar X_n)^2$$|$$\displaystyle\frac 1 {n-1}\sum_{i=1}^n (X_i-\bar X_n)^2$$|
|covariance $$\text{cov}(X_i, Y_i)$$|$$\displaystyle\frac 1 n\sum_{i=1}^n (X_i-\bar X_n)(Y_i-\bar Y_n)$$|$$\displaystyle\frac 1 {n-1}\sum_{i=1}^n (X_i-\bar X_n)(Y_i-\bar Y_n)$$|
|$$\lambda$$ in $$X_i\overset{i.i.d.}{\sim}\text{Exp}(\lambda)$$|$$\displaystyle\frac{1}{\bar X_n}$${% if jekyll.environment == "development" %}<br>(see [Stats L5 Q7](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab7)){% endif %}|$$nX_{(1)}$${% if jekyll.environment == "development" %}<br>(see [Stats HW1 Q4](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw1_u1intro/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw1_u1intro-tab4)){% endif %}|
|$$\theta$$ in $$X_i\sim\text{Unif}(0,\theta)$$|$$2\bar X_n$$|$$\displaystyle{\frac {n+1} n} X_{(n)}$${% if jekyll.environment == "development" %}<br>(see [Stats HW1 Q6](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw1_u1intro/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw1_u1intro-tab6)){% endif %}|

Consistency and bias are [different](https://en.wikipedia.org/wiki/Consistent_estimator#Bias_versus_consistency), and one does not imply the other. An estimator $$\hat\Theta_n=X_n$$ may be unbiased, but will not converge to anything as it considers the last sample only{% if jekyll.environment == "development" %} (see [Prob L20 Q4](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__20_An_introduction_to_classical_statistics/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch12-s4-tab4)){% endif %}.

Variance of an estimator $$\text{var}(\hat\Theta_n)$$ helps comparing performances of different estimators{% if jekyll.environment == "development" %} (see [Stats L4 Q4](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s02_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s02_parainference-tab4)){% endif %}, and is needed to assess the *quadratic risk*, or (more commonly) *mean squared error*, defined as expected value of quadratic bias $$\text{MSE}(\hat\Theta_n)=\mathbb E[(\hat\Theta_n-\theta)^2]$$.

Expanding the definition $$\text{MSE}(\hat\Theta_n)=\text{var}(\hat\Theta_n-\theta)+(\mathbb E[\hat\Theta_n-\theta])^2=\text{var}(\hat\Theta_n)+\text{bias}^2(\hat\Theta_n)\ge0$${% if jekyll.environment == "development" %} (see [Prob L20 Q7](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__20_An_introduction_to_classical_statistics/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch12-s4-tab7)){% endif %}, we obtain that if $$\text{MSE}(\hat\Theta_n)\rightarrow 0$$ then estimator $$\hat\Theta_n$$ is consistent ($$\hat\Theta_n\xrightarrow{\mathbb P}\theta$$){% if jekyll.environment == "development" %} (see [Stats L4 Q7](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s02_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s02_parainference-tab7)){% endif %} and unbiased ($$\text{bias}(\hat\Theta_n)=0$$).

The above can also be explained as convergence in $$L^2$$ norm implies convergence in $$L^1$$ norm, and therefore in probability (consistency){% if jekyll.environment == "development" %} (see [Stats L4 Q5](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s02_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s02_parainference-tab5)){% endif %}. Implications in the opposite directions are not true and convergence in expectation does not imply convergence in variance, because convergence in $$L^1$$ norm does not imply convergence in $$L^2$$ norm{% if jekyll.environment == "development" %} (see [Stats HW1 Q7](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw1_u1intro/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw1_u1intro-tab7) and [Stats HW1 Q8](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw1_u1intro/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw1_u1intro-tab8)){% endif %}.

## Delta method

Occasionally, the unknown parameter is a *function* of the expectation, $$\theta=g(\mu_X)$$. In such cases, CMT preserves consistency $$g(\bar X_n)\xrightarrow{\mathbb P}g(\mu_X)$$ and asymptotic normality, but not the asymptotic variance. By the Taylor's expansion, $$g(\bar X_n)=g(\mu_X)+g'(\mu_X)(\bar X_n-\mu_X)+O(\bar X_n^2)$$; therefore, $$g(\bar X_n)-g(\mu_X)\simeq g'(\bar X_n)(X_n-\mu_X)$$, which allows computing the new variance, provided that is continuously differentiable around $$\mu_X$$, which in turn may depend on the possible values of $$\theta$${% if jekyll.environment == "development" %} (see [Stats L5 Q9](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab9)){% endif %}.

$$\sqrt{n}(g(\bar X_n)-g(\mu_X))\xrightarrow{(d)}\mathcal N(0,(g'(\mu_X))^2\sigma_X^2)$$

The above is known as the **Delta method**, and even though the above example was developed around the sample mean, it works for any asymptotically Normal distribution{% if jekyll.environment == "development" %} (see [Stats L5 Q8](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab8)){% endif %}, including with non invertible $$g(\mu_X)$${% if jekyll.environment == "development" %} (see [Stats L5 Q11](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab11)){% endif %}.

## Estimator properties recap

In addition to consistency and bias, estimators could be further characterized by the following:

- **Construction approach**: generic (i.e. starting from the sample mean $$\bar X_n$$) or specifically tailored. Examples of the latter include $$X_{(1)}$$ to estimate the offset of an shifted Exponential r.v., or $$X_{(n)}$$ to estimate the maximum of a Uniform r.v.
- **Distribution**: asymptotic (i.e. known only for $$n\rightarrow\infty$$, such as $$\sqrt n (\bar X_n-\mu_X)\xrightarrow{(d)}\mathcal N(0,\sigma_X^2)$$) or non-asymptotic. The benefits of an estimator with a non-asymptotic distribution is that it can be determined for any $$n$$, including when we have only a limited size of the samples set. 
- **Variance**: where narrower variance is a synonym for faster convergence. Note that faster convergence may also stem from discontinuity existing in the PDF of the estimator.
- **Robustness**: outliers may be unavoidable during data collection, either due to measurement errors or flaws in theory that justified our statistical model. Estimators susceptible to outliers should not be our first choice, unless we can ensure their occurrence will not affect our estimator.

## Confidence Interval

Any random interval $$\mathcal I$$ which boundaries do not depend on $$\theta$${% if jekyll.environment == "development" %} (see [Stats L4 Q8](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s02_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s02_parainference-tab8)){% endif %} is defined as (**asymptotic**) **confidence interval** (CI) of level $$1-\alpha$$ if:

$$\left(\lim_{n\rightarrow\infty}\right)\mathbb P(\theta\in\mathcal I)\ge1-\alpha,\forall\theta\in\Theta$$

There are two major takeaways from the above definition:

- CI is a **random** interval that, depending on its realization, will either contain or not the unknown parameter{% if jekyll.environment == "development" %} (see [Stats L5 Q3](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab3) and [Stats L5 Q4](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab4)){% endif %}, where the expected of successes $$\mathbb E[\mathbb 1(\theta\in\mathcal I)]$$ will determine its level{% if jekyll.environment == "development" %} (see [Stats L5 Q12](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab12)){% endif %};
- the inequality sign means that CI of a given level, is **also** CI of any lower level{% if jekyll.environment == "development" %} (see [Stats L5 Q5](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab5)){% endif %}, provided that our objective is finding the *narrowest* interval satisfying that given level; and
- Unless estimator's distribution is well known for every $$n\lt\infty$$, we will need to rely on convergence theorems (CLT, Slutsky's, CMP, Delta method) to derive **asymptotic** CI for $$n\rightarrow\infty$$

Derivation of CI *could* follow the following procedure:

- Define a statistical model $$(E,\{\mathcal P_\theta\}_{\theta\in\Theta})$$ based on observations $$X_1,\dots,X_n$$, where $$\Theta\subseteq\mathbb R$${% if jekyll.environment == "development" %} (see [Stats L5 Q6](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab6)){% endif %};
- Construct an estimator $$\hat\Theta_n$$ of parameter $$\theta$$ and derive its distribution;
- Compute an interval $$\mathcal J(\theta)=\theta+[-u,v]$$ s.t. $$\mathbb P(\hat\Theta_n\in\mathcal J(\theta))\ge1-\alpha$${% if jekyll.environment == "development" %} (see [Prob L20 Q9](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__20_An_introduction_to_classical_statistics/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch12-s4-tab9) and [Stats HW1 Q5](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw1_u1intro/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw1_u1intro-tab5)){% endif %}; and
- Derive an interval $$\mathcal I(\hat\Theta_n)=\hat\Theta_n+[-v,u]$$ s.t. $$\mathbb P(\theta\in\mathcal I(\hat\Theta_n))\ge1-\alpha$${% if jekyll.environment == "development" %} (see [Prob L20 Q8](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__20_An_introduction_to_classical_statistics/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch12-s4-tab8)){% endif %}.

Note that in spite of the last manipulation, the interval $$\mathcal I(\hat\Theta_n)$$ may still depend on the unknown $$\theta$$. To address this, we may rely on one of the following three strategies:

- **Conservative** bound, particularly suitable to [bounded](/2022/01/08/discrete-random-variables.html#variance) r.v.{% if jekyll.environment == "development" %} (see [Stats L4 Q9](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s02_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s02_parainference-tab9)){% endif %};
- **Solving** the (quadratic) equation{% if jekyll.environment == "development" %} (see [Stats L4 Q10](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s02_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s02_parainference-tab10)){% endif %}; and
- **Plugging-in** the estimator $$\hat\Theta_n$$ in place of $$\theta$$, by exploiting Slutsky's theorem{% if jekyll.environment == "development" %} (see [Stats L4 Q11](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s02_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s02_parainference-tab11)){% endif %}.

Where the first two strategies will not change the (non)asymptotic nature of the interval determined before, application of plug-in strategy will necessarily lead to an *asymptotic* CI, due to the fact that Slutsky's theorem applies to convergent sequences only.

Before moving forward, bear in mind that although a CI of higher level is *wider* than a CI of lower level, the former does not necessarily contain the latter if they were derived using different strategies{% if jekyll.environment == "development" %} (see [Stats L5 Q2](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab2)){% endif %}.

## Two-sided Confidence Interval of the Mean

Considering the importance of the sample mean $$\bar X_n$$ as a consistent estimator of the expectation with a well known distribution, it is quite common to see the CI defined as $$\left[\bar X_n\pm q_{\frac\alpha 2}\frac{\hat\sigma_X}{\sqrt n}\right]$$, equivalent to:

$$\mathcal I(\bar X_n)=\left[\bar X_n-q_{\frac\alpha 2}\frac{\hat\sigma_X}{\sqrt n},\bar X_n+q_{\frac\alpha 2}\frac{\hat\sigma_X}{\sqrt n}\right]\text{, such that }\mathbb P\left(\mu_X\in\mathcal I(\bar X_n)\right)\ge1-\alpha$$

Where $$q_{\frac\alpha 2}$$ is the two sided [quantile](/2022/01/13/continuous-random-variables.html#normal) of order $$1-\alpha$${% if jekyll.environment == "development" %} (see [Prob L20 Q11](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Lec__20_An_introduction_to_classical_statistics/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch12-s4-tab11) and [Prob PS8 Q3](https://learning.edx.org/course/course-v1:MITx+6.431x+1T2020/block-v1:MITx+6.431x+1T2020+type@sequential+block@Problem_Set_8/block-v1:MITx+6.431x+1T2020+type@vertical+block@ch12-s7-tab3)){% endif %} and $$\hat\sigma_X^2$$ is either an upper bound of the variance or (unbiased) sample variance obtained from observations.

The CI is obtained by solving a (quadratic) equation is generally staggered with respect to an interval centered around the mean{% if jekyll.environment == "development" %} (see [Stats L5 Q10](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s03_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s03_parainference-tab10)){% endif %}, like in the following *indicative* examples{% if jekyll.environment == "development" %} (see [Stats HW2 Q1](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw2_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw2_u2parainf-tab1)){% endif %}.

||$$\sigma_X^2=\theta^2$$|$$\sigma_X^2=\theta$$|
|-|:-:|:-:|
|equation|$$\displaystyle\lvert g(\bar X_n)-\theta\rvert\le q_{\frac \alpha 2}\frac{\theta}{\sqrt n}$$|$$\displaystyle\lvert g(\bar X_n)-\theta\rvert\le q_{\frac \alpha 2}\frac{\sqrt\theta}{\sqrt n}$$|
|solution interval|$$\displaystyle\left[\frac{g(\bar X_n)}{\left(1\pm\frac{q_{\frac \alpha 2}}{\sqrt n}\right)}\right]$$|$$\displaystyle\left[g(\bar X_n)+\frac {q_{\frac \alpha 2}^2}{2n}\left(1\pm\sqrt{1+2g(\bar X_n)\frac{2n}{q_{\frac \alpha 2}^2}}\right)\right]$$|

The generic approach in determining a new estimator, can be as follows:

- check out the converging limit of the sample mean (WLLN);
- observe the Normal distribution and its asymptotic variance (CLT);
- apply necessary transformations to isolate the desired (unknown) parameter (CMT);
- determine the new asymptotic variance (Delta method){% if jekyll.environment == "development" %} (see [Stats HW2 Q2](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw2_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw2_u2parainf-tab2)){% endif %}; and
- compute CI of the desired level{% if jekyll.environment == "development" %} (see [Stats HW2 Q3](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw2_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw2_u2parainf-tab3)){% endif %}.

## Hypothesis Testing

Ultimate step of statistical inference is to *test* whether the (unknown) true parameter $$\theta$$ satisfies given conditions. Assume we collect **one-sample** and believe that population parameter $$\theta$$ is either in $$\Theta_0$$ or $$\Theta_1$$. This means that we have the following two hypotheses to test against each other.

$$\begin{cases}H_0:&\theta\in\Theta_0&\text{null hypothesis}\\H_1:&\theta\in\Theta_1&\text{alternative hypothesis}\end{cases}$$

The objective is in reality to look for evidence in the data to reject $$H_0$$, although lack of evidence will not mean that $$H_0$$ is true. To achieve this, assume $$H_0$$ is and compute the region where $$\hat\Theta_n$$ would fall with (low) probability $$\alpha$$. The heuristic is that if an estimator's realization $$\hat\theta_n$$ (*estimate*) falls in this (low) probaility region, then it is unlikely that $$H_0$$ explains our data and therefore can be rejected.

Occasionally, we collect **two-samples** of separate, independent populations, and test whether their respective parameters are equal or not (or if their difference is equal to a given value){% if jekyll.environment == "development" %} (see [Stats L6 Q4](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s04_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s04_parainference-tab4) and [Stats HW3 Q6](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw3_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw3_u2parainf-tab6)){% endif %}. In such cases, one sample is called **control set** (to represent some base reference) and the other **test set**.

In either case (one-sample or two-sample tests), we need to some make modeling assumptions and reduce hypotheses to yes/no questions{% if jekyll.environment == "development" %} (see [Stats L6 Q2](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s04_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s04_parainference-tab2)){% endif %}. Below definitions are useful in understanding how the test is carried out:

- **Test statistic** $$T_n(\hat\Theta_n,\theta)\in\mathbb R$$ is sort of distance function between $$\hat\Theta_n$$ and a reference $$\theta$$r. $$T_n$$ is a [pivot](https://en.wikipedia.org/wiki/Pivotal_quantity) if we are able to write it in such a way that its distribution under the $$H_0$$ is known and does not depend on any other parameters ([such as](https://en.wikipedia.org/wiki/Test_statistic) $$z$$-distribution, $$t$$-distribution or $$\chi^2$$-distribution). For example if $$\hat\Theta_n=\bar X_n$$, then a possible $$T_n(\bar X_n,\mu_X)=\sqrt n\frac{\bar X_n-\mu_X}{\sigma_X^2}$$, which we know converges in distribution to $$Z\sim\mathcal N(0,1)$$;
- **Test** $$\psi_\alpha=\{0,1\}$$ is a *statistic* of the form $$\psi_\alpha=\mathbb 1(T_n(\hat\Theta_n,\theta)\gt c_\alpha)$$, where $$\theta\in\Theta_0$${% if jekyll.environment == "development" %} (see [Stats HW3 Q7](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw3_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw3_u2parainf-tab7)){% endif %}. Recall that $$T_n$$ is a distance function and $$\psi_\alpha=1$$ means that $$T_n$$ exceeded certain threshold and therefore we reject $$H_0$$. Note that when $$H_1:\theta\neq\theta_0$$, our $$T_n$$ will be based on the *absolute* distance $$\lvert\hat\Theta_n-\theta_0\rvert$$ and the test will be **two-sided**{% if jekyll.environment == "development" %} (see [Stats HW3 Q2](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw3_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw3_u2parainf-tab2)){% endif %}. There are examples of composite tests, but the results in terms of thresholds would be the same as in case of multiple **one-sided** exercises{% if jekyll.environment == "development" %} (see [Stats HW3 Q8](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw3_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw3_u2parainf-tab8)){% endif %};
- **Type 1 error** $$\alpha_\psi(\theta)\in[0,\alpha]$$ is the probability of rejecting $$H_0$$ when $$H_0$$ is true. More formally, $$\alpha_\psi(\theta):\Theta_0\rightarrow[0,\alpha];\theta\mapsto\mathbb P_{\theta}(\psi=1)$$, which is equivalent to $$\alpha_\psi(\theta)=\mathbb P(T_n(\hat\Theta_n,\theta)\gt c_\alpha)$$, where $$\theta\in\Theta_0$${% if jekyll.environment == "development" %} (see [Stats L6 Q5](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@u02s04_parainference/block-v1:MITx+18.6501x+2T2021+type@vertical+block@u02s04_parainference-tab5)){% endif %}. Upper bound $$\alpha$$ is the (**asymptotic**) **level** and is a design parameters that defines the test (and not the other way around). In general, $$\alpha=\alpha_\psi(\theta_0)$$ where $$\theta_0$$ that is at the *boundary* between $$\Theta_0$$ and $$\Theta_1$$, because this is the point of the highest ambiguity;
- **Threshold** $$c_\alpha\in\mathbb R$$ is the main criteria that defines when the test should reject $$H_0$$ and it depends on the designated level $$\alpha$$. If $$T_n$$ is a pivot, asymptotically distributed according to $$\mathcal N(0,1)$$, then $$\mathbb P(T_n\gt c_\alpha)$$ will converge to $$1-\Phi(c_\alpha)$$, as $$n\rightarrow\infty$$. Since by design we want $$\mathbb P(T_n\gt c_\alpha)\le\alpha$$, we derive that $$c_\alpha=q_\alpha$$, [quantile](/2022/01/13/continuous-random-variables.html#normal) of order $$1-\alpha$$ of a standard normal distribution, is the lowest threshold that would satisfy such relation ($$c_\alpha=q_{\frac\alpha2}$$ in case of a *two-sided* test);
- **Rejection region** $$R_{\psi_\alpha}\in E^n$$ is a subspace elements of which satisfy $$\psi(x_1,\dots,x_n)=1$$. It can also be seen in terms of $$T_n$$ as $$R_{\psi_\alpha}=\{T_n\gt c_\alpha\}$$, or $$\hat\Theta_n$$ as $$R_{\psi_\alpha}(\theta)=\{T_n(\hat\Theta_n,\theta)\gt c_\alpha\}$$, with $$\theta\in\Theta_0$$. This last manipulation highlights the duality between CI and the rejection region and in particular that the **acceptance region** (complementary to rejection region) can be assimilated as a CI, and assessment of $$H_1:\theta\neq\theta_0$$ can be promptly translated into $$\psi_\alpha=\mathbb 1(\theta_0\notin\mathcal I(\hat\Theta_n))$${% if jekyll.environment == "development" %} (see [Stats HW3 Q4](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw3_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw3_u2parainf-tab4)){% endif %};
- **Type 2 error** $$\beta_\psi(\theta)\in[0,1]$$ is the probability of not rejecting $$H_0$$ when $$H_1$$ is true. More formally, $$\beta_\psi(\theta):\Theta_1\rightarrow[0,1];\theta\mapsto\mathbb P_{\theta}(\psi=0)$$. Recall that $$H_1$$ does not play a symmetric role (the data is only used to disprove $$H_0$$), and $$\beta_\psi$$ is determined only after rejection and acceptance regions have been set{% if jekyll.environment == "development" %} (see [Stats HW3 Q3](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw3_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw3_u2parainf-tab3)){% endif %}. Analogously to $$\alpha_\psi(\theta)$$, the highest $$\beta_\psi(\theta)$$ occurs in the proximity of $$\theta_0\in\Theta_0$$ when it is also supremum or infimum of $$\Theta_1$$. In such case, $$\sup_{\theta\in\Theta_1}\beta_\psi(\theta)$$ would be equal to $$1-\alpha_\psi(\theta_0)=1-\alpha$$;
- **Power** $$\pi_\psi=\inf_{\theta\in\Theta_1}(1-\beta_\psi(\theta))$$ and is the probability that test $$\psi$$ rejects $$H_0$$ when $$H_1$$ is true. Considering that $$1-\beta_\psi(\theta)$$ is minimum when $$\beta_\psi(\theta)$$ is maximum, we conclude that $$\pi_\psi=\alpha$$ when $$\Theta_0$$ and $$\Theta_1$$ are two, contiguous partitions of the real line. In addition, bearing in mind that $$\beta_\psi$$ is better tolerated than $$\alpha_\psi$$, we see that the major drawback of a high $$\beta_\psi(\theta)$$ is that we are losing power of *detecting* $$H_1$$ when it is actually true;
- (**Asymptotic**) $$p$$**-value** is the smallest (asymptotic) level $$\alpha$$ at which $$\psi_\alpha$$ rejects $$H_0$$. This definition may seem counterintuitive, since $$p$$-value is just the probability of obtaining $$T_n$$ at least as extreme as $$t_n$$ actually observed, assuming $$H_0$$ is true (i.e. $$p=\mathbb P_{\theta\in\Theta_0}(\lvert T_n\rvert\gt t_n)$$){% if jekyll.environment == "development" %} (see [Stats HW3 Q5](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw3_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw3_u2parainf-tab5)){% endif %}, but it leverages on the fact that $$p$$-value is *random* and depends on the sample---and that based on the actual realization, one can arbitrarily set the level when to accept or reject $$H_0$${% if jekyll.environment == "development" %} (see [Stats HW3 Q1](https://learning.edx.org/course/course-v1:MITx+18.6501x+2T2021/block-v1:MITx+18.6501x+2T2021+type@sequential+block@hw3_u2parainf/block-v1:MITx+18.6501x+2T2021+type@vertical+block@hw3_u2parainf-tab1)){% endif %}.

Find below a summary table of the error types and their probabilities.

|test outcome|$$H_0$$ is true|$$H_1$$ is true|
|:-:|:-:|:-:|
|$$H_0$$ is **not** rejected|Correct inference<br>(true negative)<br>$$1-\alpha$$|Wrong inference<br>(false negative)<br>$$\beta$$|
|$$H_0$$ is rejected|Wrong inference<br>(false positive)<br>$$\alpha$$|Correct inference<br>(true positive)<br>$$\pi$$|

Go back to the [syllabi breakdown](/2022/01/02/prob-and-stats-syllabi.html).

***

## Back-up

## Variance empirical estimator

Assume $$X_i\overset{i.i.d.}{\sim}\mathcal P$$, where $$\mathbb E[X_i]=\mu_X$$ and $$\text{var}(X_i)=\sigma_X^2$$. Assume also $$\bar X_n=\frac 1 n\sum_{i=1}^n X_i$$, where $$\mathbb E[\bar X_n]=\mu_X$$ and $$\text{var}(\bar X_n)=\frac{\sigma_X^2}{n}$$. Now, observe that $$\mathbb E[X_i^2]=\sigma_X^2+\mu_X^2$$, $$\mathbb E[\bar X_n^2]=\frac{\sigma_X^2}{n}+\mu_X^2$$ and $$\mathbb E[X_i\bar X_n]=\frac 1 n\mathbb E[X_i^2+X_i\sum_{i\neq j}X_j]=\mathbb E[\bar X_n^2]$$.

Based on the above, we compute $$\mathbb E[(X_i-\bar X_n)^2]=\mathbb E[X_i^2-2X_i\bar X_n+\bar X_n^2]=\frac{n-1}{n}\sigma_X^2$$ and notice that the empirical variance estimator $$\hat\sigma_X^2=\frac 1 n\sum_{i=1}^n(X_i-\bar X_n)^2$$ is biased, since $$\mathbb E[\hat\sigma_X^2]=\frac{n-1} n \sigma_X^2$$. The bias can be addressed by defining an unbiased estimator $$s_X^2=\frac 1 {n-1}\sum_{i=1}^n(X_i-\bar X_n)^2$$. Exactly same arguments can be used to derive unbiased estimator of covariance.